[{"title":"Destructuring in Kotlin","date":"","description":"","body":"Destructuring is a programming concept that entails breaking down a complex data structure into its individual components and assigning these components to individual variables directly in a single statement.\nAs many other modern languages support it, but Kotlin, at least when compared to JavaScript, has a different behavior when used on data classes.\nThe purpose of this text is to talk about this difference, and how to prevent, as earlier as possible, mistakes that its usage could cause to your code.\nUnderstanding Destructuring Declarations Destructuring allows you to decompose an object into a set of variables. For example:\ndata class Item(val name: String, val price: Double) val rice = User(\u0026#34;Rice\u0026#34;, 2.13) val (name, price) = rice In this code, a new Item instance was created and destructured into two variables that represent its attributes.\nIt was achieved by using the following statement:\nval (name, price) = rice In this case, a data class was used, but it is also possible to use it in other data structures, like Arrays, Lists or other classes.\nval list = listOf(\u0026#34;element 1\u0026#34;, \u0026#34;element 2\u0026#34;) val (l1, l2) = list val array = arrayOf(\u0026#34;element 1\u0026#34;, \u0026#34;element 2\u0026#34;, \u0026#34;element 3\u0026#34;) val (a1, a2) = array As it was possible to notice on the array example, it isn\u0026rsquo;t mandatory to have all elements assigned to variables. Non-mapped values are simply ignored.\nImportant: mapping more elements than existing on the list will throw the java.lang.ArrayIndexOutOfBoundsException\nThe Order Matters The most significant difference between destructuring in Kotlin, when compared to JavaScript is that the ordering of the elements is important.\nIn JavaScript, as it is in Kotlin, destructuring a collection is order-based, which means that the first variable declaration will be assigned to the first element of the collection, the second variable to the second element, and so on…\nThe difference between those two languages is on destructuring objects, where in JavaScript, it extracts the properties from the object using its declared name and Kotlin extracts based on the declaration order.\nIn JavaScript, by having the following object:\nlet rice = {name: \u0026#34;Rice\u0026#34;, price: 2.13}; It is possible to destruct it by using both statement, and both of them will always have the same result:\nlet {name, price} = rice; // name = \u0026#34;Rice\u0026#34;, price = 2.13 let {price, name} = rice; // price = 2.13, name = \u0026#34;Rice\u0026#34; In Kotlin, by having the Rice object:\ndata class Item(val name: String, val price: Double) val rice = User(\u0026#34;Rice\u0026#34;, 2.13) The declaration order matters when destructuring it:\nval (name, price) = rice // name = \u0026#34;Rice\u0026#34;, price = 2.13 val (price, name) = rice // price = \u0026#34;Rice\u0026#34;, name = 2.13 As you can see, the order in which each attribute of the Item is declared directly impacts its assignation when destructuring it.\nType Enforcement As you can imagine, this can cause some unwanted behavior on your application by using it, and you could be thinking if it is possible to change its behavior, and the answer is: no.\nAlthough it isn\u0026rsquo;t possible to change, understanding this behavior can help you prevent issues related to it, by having test cases that explicitly cover mistakenly changes related to the used fields and also, when possible, enforcing the data type.\nEnforcing data type, makes changes like this noticeable at compilation time.\nBy changing the attribution shown earlier in this text, you can have a compilation error if the used data class has its field ordering changed.\nval (name: String, price: Double) = rice // name = \u0026#34;Rice\u0026#34;, price = 2.13 So, if someone changes the Item to have the price attribute declared first, the typing will not match, and the compilation will not succeed.\nAs you can imagine, it didn\u0026rsquo;t prevent all cases because if these two fields have the same data type, the compilation will succeed, since they still match. For cases like this, having good test cases is the only effective solution.\nEnforcing data type only brings to your code another validation, which can be useful and, usually, easier to identify the exact part of the code that got impacted by a change.\nUsing it wisely While destructuring declarations can make code more brief and expressive, avoid overusing them. Overuse can make the code harder to understand, especially for those not familiar with the feature.\nAlways prioritize readability and maintainability, and remember, just because you can, doesn\u0026rsquo;t always mean you should.\nIn conclusion, Kotlin\u0026rsquo;s destructuring declarations are a great tool to simplify your code and make it more expressive when used judiciously. They provide a way to destructure an object into several variables, making the code more concise and readable. However, like all tools, they should be used appropriately and with an understanding of their underlying mechanics.\n","ref":"/posts/2023-11/destructuring-in-kotlin/"},{"title":"Contract-First VS Code-First API Documentation","date":"","description":"","body":"API development is a crucial part of modern software development. Designing clear, robust, and easy-to-use APIs not only optimizes the development process but also enhances the developer experience. Various methodologies exist for API creation, the two significant approaches are the Contract-First and Code-First methods. This article explores these two approaches in depth.\nContract-First API Documentation Contract-first API development kicks off with the API definition. Developers outline all of the API endpoints, the request-response models, the data types, and any possible status codes. The driving principle here is to write an API contract before any code is written. This contract usually comes in a machine-readable format like Swagger (OpenAPI Spec), RAML, and API Blueprint.\nCode-First API Documentation The code-first approach, as the name suggests, involves writing code before any contractual agreement is established. Developers start building methods, libraries, or classes, and from the implementing codebase, the API documentation is generated.\nConclusion Both of these techniques have their strengths and weaknesses. Contract-first ensures an agreement and understanding across teams before coding commences, but it can slow things down initially. Code-first gets you into the code quicker but may cause issues down the line if the API design isn\u0026rsquo;t clearly communicated or understood. As an API developer, your choice between these two approaches should depend on your project\u0026rsquo;s needs, team communication, and the development workflow.\n","ref":"/posts/2023-10/contract-first-vs-code-first-api-documentation/"},{"title":"CAP Theorem","date":"","description":"","body":"CAP theorem is a fundamental principle in distributed database systems that states only two out of the three: consistency, availability, and partition tolerance, can be achieved at any given time.\nConsistency In the context of the CAP theorem, consistency means that every read operation receives the most recent write or an error. All database nodes will hold the same data at the same time.\nAvailability Availability means that each request made to the system either receives a valid response or error, regardless of the state of the system. Every non-failed node returns a response in a reasonable amount of time, therefore maximizing uptime.\nPartition Tolerance Partition tolerance explains the system\u0026rsquo;s ability to continue working despite physical network splits. It means the system as a whole can tolerate network failures that partition the system.\nTrade-offs In reality, no distributed system can simultaneously provide more than two of these three guarantees. Thus, in the presence of a network partition, one has to choose between consistency and availability.\nConclusion In practice, the CAP theorem implies that in distributed systems, there are trade-offs between consistency, availability, and partition tolerance. Specifically, a system can only guarantee two out of these three aspects at any given moment. Therefore, system architects must carefully consider their system\u0026rsquo;s requirements and use-cases before deciding on the appropriate balance.\n","ref":"/posts/2023-09/cap-theorem/"},{"title":"Installing and using minikube","date":"","description":"","body":"In my current project, we use Kubernetes massively and with that, I needed to perform tests more than once to verify that my deployment file was correct. Although there are environments that I could use to test, I decided to use minikube, in order to carry out tests locally and not risk impacting other users/applications.\nInstallation The minikube package is available for macOS, Linux and Windows platforms, however, I will only demonstrate how to install it on macOS using Homebrew. If you want to install on other platforms or with a different method, just check the options here.\nInstallation on macOS with Homebrew The minikube package is available on Homebrew and can be installed using the following command:\nbrew install minikube This way, not only minikube will be installed, but also the kubernetes-cli package, necessary to access Kubernetes resources through the terminal.\nCluster initialization To initialize a Kubernetes cluster using minikube, just open a terminal and run the following command:\nminikube start With this command, a cluster will be initialized and also the local settings to access via the kubectl command will be defined.\nThe creation of this cluster, preferably, will be in a Docker container, however, if this is not available, other methods can be used. The minikube command is in charge of identifying the available methods (drivers) and using them.\nIt is possible to specify which driver to use, for that, just add an argument specifying which driver you want to use, as in the following example, in which the use of Hyperkit is specified.\n# minikube start --driver=\u0026lt;driver name\u0026gt; # eg: minikube start --driver=hyperkit There is also the option of defining a driver as default, so that whenever the cluster is created, this is the method used. To do so, just run the following command, changing it to the desired driver.\n# minikube config set driver \u0026lt;driver name\u0026gt; # It is.: minikube config set driver hyperkit For more details on supported drivers, I recommend checking the official documentation, which also details differences, customizations, and limitations for each driver.\nAccess to the dashboard (web interface) The cluster created with minikube provides a dashboard which can be accessed using the following command:\nminikube dashboard Executing this command should automatically open your default browser, however, if it happens, in the terminal output it is possible to identify which URL should be used to access the dashboard.\n💡 This *dashboard* allows you to view, create, modify and remove the resources available in the *cluster* through a web interface, as is possible when using the client via a terminal (*`kubectl`*) Access to a service or ingress When creating a service of type LoadBalancer or even creating an Ingress, to access it it is necessary to execute the following command in terminal:\nminikube tunnel For the specific case of Ingress, it is still necessary to install an addon in Kubernetes. To do this, just run the following command:\nminikube addons enable ingress Stop or Remove the cluster The process of stopping the cluster can be performed using the following command in a terminal:\nminikube stop This command only pauses the cluster and, consequently, releases the used resources (CPU/Memory) to the host. When running the minikube start command again, this same cluster is recovered, with all the modifications made so far.\nIf you want to remove the cluster, the command to be executed will be:\nminikube delete This command will also remove all persisted data, ie when the minikube start command is executed, a new cluster will be created.\nConclusion Although not detailed in this text, several other features and customizations are available in minikube, some in addons format, which can be seen using the following command:\nminikube addons list In summary, minikube is useful for anyone who wants or needs to carry out tests in a cluster Kubernetes locally, because, in addition to being easy to install, it is also possible to install several extensions that can greatly facilitate its development and/or or application testing.\n","ref":"/posts/2023-07/installing-and-using-minikube/"},{"title":"What is a Reverse Proxy?","date":"","description":"","body":"A reverse proxy is a server that has as its main functionality to receive requests from an external network, usually the Internet, and forward them to servers on an internal network.\nThis server acts as the user\u0026rsquo;s entry point to your application, which allows you to perform various types of customization on the received request, such as changing the request parameters or rewriting the path (path) in a REST request.\nLoad balancing is also one of the capabilities of these servers, where they can not only distribute the requests among the various instances of an application, but also send them to other instances according to the parameters received in the request.\nFor example, when a request is made to the endpoint /search, that request can be forwarded to the instances of a search service, whereas when it is /users, the requests go to the users service.\nThis allows not only centralizing the access address to the available services, where everyone uses the same domain (host), but also allows load balancing between the various instances of each service.\nSeveral other functionalities can be applied to these servers, some solutions have more, others less customizations/functionalities. As an example of applications for this purpose, we can mention:\nNGINX HAProxy Conclusion As you can see, a reverse proxy can improve not only the availability and scalability of an application, but also load balancing between applications. Not only that, but with its use it is also possible to centralize in a single address (URL) all available services, creating what we can call an API Gateway.\nOther types of resources can be added at this layer, such as caching, authentication/authorization, certificate checks (TLS, mTLS), as well as improving the security of the services, since this is not available directly.\nLinks What Is a Reverse Proxy Server? What is a reverse proxy? | Proxy servers explained Reverse proxy What is Reverse Proxy Server/ What is the difference between Proxy and Reverse Proxy? | How To Use Reverse Proxy for Access Management Control ","ref":"/posts/2022-10/what-is-a-reverse-proxy/"},{"title":"What are Flaky Tests and why fix them!","date":"","description":"","body":"Building tests is surely one of the most difficult and time-consuming tasks for a developer, in addition to usually generating more lines of code than the added functionality. This is due to the various scenarios that need, or should, be verified.\nFlaky Tests, are basically test cases created to verify a scenario, however, they can randomly present success and failure results, without having undergone any change, neither in the test case nor in the tested code.\nBasically, they are those test cases that fail for no reason and, on re-execution, complete successfully.\nSeveral reasons can cause a test case to be inconsistent, let\u0026rsquo;s address some of the main causes I\u0026rsquo;ve experienced below.\nWhen the order of execution matters It is common to build a class and add methods for the different cases to be tested, however, especially when we access databases or classes that store the state, the execution of a test case can change these persisted data and, consequently, impact the desired result in the next test run.\nPerhaps this is one of the easiest cases to solve, where we can just define the order of execution of these test cases, or even perform a cleaning and preparation of the data before the execution of each test case.\nIn particular, I understand that a test case should not impact another case and I usually tend to use data cleaning and preparation routines before executing each case. However, I understand that in some cases, defining an execution order could greatly reduce the effort required to build test cases.\nAn example where the order might make sense would be to test a CRUD, where you can create a first case where we create the resource, in the second we retrieve it, in the third we would update it and finally we would delete it. As for business rules validation cases, such as performing calculations or other complex checks, I believe that cleaning and preparation routines are the best option, as it simplifies the understanding of the initial state without the need to analyze the previous test case, disregarding , that many times, the previously executed test does not have much relation with the initial state to be tested in the current case.\nI don\u0026rsquo;t believe there is a right or wrong, but different approaches. The most important thing is to understand their differences and identify where a solution can best be applied.\nDates and time zones are always tricky If you have a team that works in different locations, with different time zones, or your CI/CD is in a different time zone than yours, there is a high chance that you have already suffered from test problems that validate dates. This problem is also common to happen in leap years or beginnings of month.\nThere are several ways to solve this problem, but in general, it consists of modifying how the validation of dates is performed, in order to be able to control the \u0026ldquo;clock\u0026rdquo;, that is, define the current moment and, consequently, have control of validations involving date. .\nIn Java, for example, when retrieving the current moment, it is common to use the following block:\nfinal Instant now = Instant.now(); However, it is possible to control the \u0026ldquo;clock\u0026rdquo; by changing to the following code:\nfinal Instant now = Instant.now(clock); With this, in our test cases, we just build a mock of java.time.Clock to return a specific date, that is, we have control of the \u0026ldquo;clock\u0026rdquo; and ensure validation of a specific scenario in the test case , regardless of where in the time zone or day the due case is being executed.\nSynchronization in Async The use of asynchronous methods has increasingly become routine in application development, given that in many cases, some processes could be executed in parallel, without blocking other operations or even user actions.\nHowever, ensuring that an asynchronous method performed some operation as desired is often a difficult task, since, as it executes at another time, we have no control over when it will be performed, which, depending on the computational capacity, can lead to failures by advance checks. For example, an asynchronous method should persist information in the database, however, at the moment we check, this insertion has not yet occurred, resulting in a failure, however, some time later, the information is inserted correctly.\nThe difficulty in this type of test is to identify the moment to verify that something has happened, since we have no way of controlling when it will occur. There are several ways to try to ensure that processing has completed before performing the checks, the most common is also the way that I believe is the one we should avoid. That is, block the process for a certain time, using, for example, Java\u0026rsquo;s Thread.sleep(long).\nThe problem with this approach is that, if the processing completes in 1 second, but we define 60 seconds there, the test case will always take the maximum time to complete and when we replicate this approach for all tests, we end up multiplying the time needed for execution of the tests.\nI believe that not all the tools that you may be using have support for checking asynchronous processes, and in certain cases, the use of the aforementioned method may be acceptable, but whenever possible, we should opt for optimized approaches that aim not to block the process. for a specific time, but up to a maximum time limit, performing the checks when it is completed in advance.\nAn example of a tool that supports this type of validation would be Mockito which offers verification using timeout(long), which ends the verification if it occurs within the specified time or fails if it does not.\nEnd-to-end tests As it is a type of test that integrates with other applications/services, it is naturally a Flaky Test and we hardly have an ideal solution for these cases, consequently requiring analysis for confirmation.\nPrecisely because it integrates with other systems, we have several factors involved that can cause a given test case not to have the expected result, which consequently generates a validation failure. A common example in this type of test would be a possible network intermittence or a simple unavailability of the consumed service, which makes the case to be tested not possible, resulting in failure.\nAs said before, there is not much to do in these cases, building a resilient application, with retries can minimize these problems, however, it is not a guarantee, depending on the cause of the problem. This is exactly why, in these cases, it is always important to analyze the problem to identify if it is something new, that is, due to a code change, or something external that we have no way to control.\nOf course, most of these tests could be converted to tests that use tools that would give us control over the scenarios, allowing us to create simulations to test some specific/necessary case. Often, the use of a real database could be replaced by an in-memory database or an integration to a REST API could be done for a mock created using WireMock instead of the \u0026ldquo;real\u0026rdquo; endpoint.\nIn general, these tests are very time consuming in execution and require another considerable time of analysis to confirm if it was a problem introduced in the change or some kind of external intermittence. Reducing the amount of this type of test, I believe, is one of the best solutions to minimize the occurrence of unexpected failures.\n⚠️ Reducing is not removing! The process of reducing these tests normally consists of adapting these cases to the use of *mocks* and in certain cases, their removal, but this should only be done if another existing test is covering this specific case. Why fix? Tests have the function of guaranteeing that a desired behavior is actually happening, that is, if something does what it should do and, consequently, if a change has not impacted this behavior, thus generating problems that can directly impact the users of this system. Another benefit that systems with good code coverage brings is for code refactoring, because, as we have good test cases, we can easily identify when some modification has changed the behavior of the application, so the change must be reanalyzed and in extreme cases, discarded.\nUnderstanding the advantages mentioned above, when we have test cases that fail randomly, regardless of whether we have changed something or not, we end up losing this confidence in the test cases, because we are never sure whether or not we create a problem in the application with the change that we make. we did, consequently consuming a lot of analysis time to confirm that the test failed due to the change or some test considered Flaky.\nTests consume execution time, when we come across Flaky Tests, we end up having to execute the test cases more than once for them to finish successfully and this, depending on the case, can consume a large part of your day, especially when we use CI/CD that blocks the merging of our code, when it doesn\u0026rsquo;t successfully execute all the existing test cases, which consequently generates frustration for the developer.\nIt is not always easy to identify these cases, but correcting them is important to guarantee confidence in the test cases, bring security in corrections and in the development of new features, in addition to improving productivity, since we have a guarantee that, when our tests fail, it\u0026rsquo;s in fact some new problem that we created and not a burst that, when rerun, disappears.\n","ref":"/posts/2022-08/what-are-flaky-tests-and-why-fix-them/"},{"title":"GnuPG: how to backup and restore your keys?","date":"","description":"","body":"Recently, I had to transfer my private keys to a different computer. Since this looks like a trivial task (it was my first time doing it), I have decided to search for a \u0026ldquo;how-to\u0026rdquo; blog post that details the steps required for this task. After a few minutes, I got shocked that it seems to be a task that people don\u0026rsquo;t usually do, they seem to transfer only one key at once, never the whole key-ring.\nIt took more time than expected to find the commands required to transfer all my key-ring, probably more than it would take if I transfer each key individually, but I finally have it.\nExporting the keys The following command generates a single file that contains all your keys:\ngpg --export-options backup -o ~/keyring.gpg --export-secret-keys The export process will ask for the passphrase of your keys and generates a single file keyring.gpg in the user $HOME directory.\nImporting the keys After the exporting process succeeds, copy the generated file to its destination and proceed with the importing process, using the following command:\ngpg --import ~/keyring.gpg As in the exporting process, the passphrases are mandatory in the import process.\nList the keys Simple as is, your keys are now available, and you can check them using the following command:\ngpg --list-secret-keys There is probably a better way to transfer and store your keys, but this process could be useful for you and is simple to execute.\nThe file that was generated in the export process can be stored, just be careful to store it in a safe and trusted location. Remember: There are your keys!\nReferences: GPG Input and Output (Using the GNU Privacy Guard) ","ref":"/posts/2022-07/gnupg-how-to-backup-and-restore-your-keys/"},{"title":"What is a CDN (Content Delivery Network)?","date":"","description":"","body":"A Content Delivery Network, can be described as a set of servers, distributed globally, with the purpose of providing content to users in an optimized way and in the shortest possible time.\nThe CDN servers are distributed in several locations, some of these servers tend to be located closer to the destination user than the origin server, which makes the distance that the data travel (physically) is smaller, consequently, delivered more fast.\nWhat can be distributed and stored The main objective is to store static content, that is, all content that, once generated, is no longer altered. Examples of this content, when we talk about a website, for example, are HTML, CSS, images and JavaScript files.\nAlthough some files are exemplified above, other formats can be distributed without major limitations, such as videos, PDF, TXT, etc.\n💡 It is important to say that some providers, such as [AWS](https://aws.amazon.com/caching/cdn/), offer solutions that allow you to make dynamically generated content available on your CDN, however, these are usually not stored on its servers, only trafficked through the \"internal\" network, functioning as an intermediary (*proxy*) between source (server) and destination (user). Content distribution and replication Due to the distributed nature of a CDN, the content to be provided by it needs to be available to all instances of that network and, basically, there are two ways to carry out the distribution of this content. They are known as PULL and PUSH where the main differences between them is in the way data is made available on the CDN.\nPULL Model In this model, whenever content is requested by the user, the CDN checks if it already has the content on its servers, if not, it makes a request to the origin server, stores the data in its network (cache) and then returns the data to the user. requested.\nCDN example (PULL model)\nThe main advantage of this model lies in its low implementation complexity, as it is not necessary to make the content to be provided by the CDN available before the user\u0026rsquo;s first request, as it will be responsible for retrieving this content as needed, that is, as requested. by users. On the other hand, as the content will not be present on the CDN servers during the first requests, they will have a longer response time, given the need to recover this content from the origin servers.\nPUSH Model Unlike the PULL model, here the content to be made available must be available on the CDN servers before the requests are made, that is, the CDN will not be responsible for retrieving the content from an origin, but the developer or application must store these contents manually or by automated processes.\nCDN example (PUSH model)\nIn this model, we have a greater complexity in its implementation, since the content needs to be submitted to the CDN servers in advance, either by automated or manual processes. The automation of these processes can be quite complex and different processes are required for each existing application, which can make automation an extremely complex and costly process. When performed manually, human errors can occur, such as, for example, forgetting to submit some important content, causing an unexpected failure in the execution of the application.\nOn the other hand, one of the biggest benefits obtained in this model is the consistency of the response, given that the CDN never needs to retrieve data from anywhere other than its own servers, the response time tends to be consistent and low.\nAnother advantage in this mode is the ability to make systems available without the need for a server, using only the CDN. For example, it is possible to build and provide a website that does not depend on external services, just making the HTML, CSS and JavaScript files available on the CDN, without any need for dedicated servers.\nConclusion In general, despite adding a certain level of complexity to applications, its use is normally valid and recommended, since, given the proximity of a CDN provider\u0026rsquo;s servers to the users of its application, the data requested by them are normally delivered with low time, regardless of your geographic location, without the need to add your application servers around the world, allowing you to scale your application/service without necessarily increasing the operational cost.\nIt is important to say that the purpose of the text was to describe what a CDN is and not detail specific services from specific providers, such as, for example, AWS, which allows dynamic content to be made available. Each provider tends to have different characteristics, either by providing new features, such as geographic availability, and it is always advisable to identify them and validate the one that best suits your needs.\nI\u0026rsquo;ll stop here, thanks for reading, if you have questions or even find errors in the text, feel free to contact me / let me know.\nReferences What is a CDN? | How do CDNs work? What is CDN? | How Does a Content Delivery Network Work? AWS: Content Delivery Network (CDN) Caching Cloud CDN: Content Delivery Network | Google Cloud ","ref":"/posts/2022-04/what-is-a-cdn-cdn-content-delivery-network/"},{"title":"Load Balancing: What is a load balancer?","date":"","description":"","body":"Widely used in applications with high volume of simultaneous access, a load balancer is a computational resource used to perform load distribution between two or more servers of an application.\nThey are usually implemented using machines dedicated to this work, which can be physical or virtual, or by “software”.\nBenefits Regardless of the model to be implemented, they aim to distribute the processing of an application among the available instances, seeking to optimize the use of computational resources, such as network, processing (CPU), memory usage, among others.\nThis distribution of resources reduces the risk that the instances of an application are overloaded, consequently optimizing response times at times of high number of simultaneous accesses to the application.\nAnother benefit that comes from using load balancing is the increase in availability (resilience), as they use application sanity validation, which means that in cases where one or more instances are unavailable, no traffic is forwarded to them, thus helping the end user not to perceive this unavailability.\nBalancing models A load balancer can operate in two network layers of the OSI model, layer 4 (transport) and layer 7 (application). Balancers built in the transport layer tend to require less processing power to perform the task, however, they have access to less request information than application layer implementations. Although more expensive, in relation to the need for processing, layer 7 deployments tend to bring many benefits and with increasingly affordable costs, its adoption has grown a lot in recent years.\n💡 It is important to note that there are cases where an application layer balancer would not be justified (cost x benefit), as the requests to be processed never require control that is not obtained with transport layer deployments. Layer 4: Transport This balancing model works at the transport layer, that is, routing decisions are based on data available there, that is, protocol information, such as TCP or UDP, destination and source IP addressing. Based on this information, the packet is forwarded through a resource called NAT.\nIt is important to note that balancing at this layer is done purely with this information, that is, the packet content is not intercepted and/or analyzed during forwarding, which can make it difficult or even disable the creation of rules that guarantee that the same user have your requests always directed to the same instance of an application, which can be problematic when, for example, there is a need for user session control.\nLayer 7: Application This balancing model offers more resources for balancing decisions and this is due to the fact that it works at the highest layer of the OSI model. As it is in this layer, it is possible to analyze, in addition to the existing information in layer 4, as well as other information related to the received request to decide which instance the request will be forwarded to. For example, in an HTTP request balancing, we can check information contained in the header to decide the destination server, this allows us to control the distribution in the use of resources, but also allows us to guarantee other controls, such as, for example, guarantee that all requests of a given user are always processed by the same application instance.\nBalancing Strategies The principle of a load balancer is to distribute the requests among the servers available for processing the request, however, in practically all implementations, we can define how these requests will be distributed among the instances of our application. This decision is made by pre-defined algorithms, but it is usually also possible to create custom rules.\nIn this text I will describe two of the algorithms that I believe to be the most used, however, several others exist and it is always recommended to evaluate them before choosing which one to use in your application.\nRound Robin This algorithm is normally the standard used by many balancing solutions on the market. In it, each request is forwarded to a different instance among the available ones, following a standardized and continuous order.\n💡 Example: Considering 3 destination servers, the first request will be forwarded to the first server, the second to the second server and the third to the third server. From the fourth request, the list of servers is restarted, that is, it would be forwarded to the first server, the next to the second and so on. Although effective for most cases, this algorithm can cause overload on one of the servers, as it does not consider pending requests to forward a request or not. That is, it can be problematic for cases where most requests processed by the application have very different times between them and require intensive use of processing to complete, which means that one of the servers may have more requests being processed at a given moment than others, consequently degrading application performance.\nLeast Connection This algorithm forwards to servers using a simple analysis model, which consists of verifying which server among those available has the least amount of requests at the moment.\n💡 Example: Considering 3 destination servers, where we currently have the first with 5 connections, the second with 3 and the third with 4, in this model, the next request would be forwarded to the second server, as it is the one with the fewest active connections at the moment and that logic would be applied to all new requests. In other words, it is a very efficient model in relation to load distribution and usually superior in performance, as the server with the lowest number of requests will always be the one chosen to receive the next request. However, it can still be impacted by the distribution of requests, where a server, despite having fewer connections, all require high processing power, thus degrading all active requests on this server.\nExisting solutions There are several devices and applications on the market in order to offer a load balancing solution. cloud providers such as Amazon AWS and Google Cloud, also have native solutions available, which can facilitate their implementation when using these providers.\nIn the Open Source world, there are several solutions created for this purpose, each with its advantages and disadvantages.\nHere is a list of what I believe to be the main solutions currently and their respective layers of action:\nHAProxy (layer 4 and 7) nginx (layer 4 and 7) Seesaw (Layer 4) Traefik (layer 7) Conclusion Briefly, a load balancer (load balancer) is a computational resource, which can be a device or an application, responsible for distributing the processing between the different instances of an application through defined strategies.\nIt acts as an intermediary (proxy), receiving and directing requests according to defined rules and aiming not only to increase the processing capacity of the application but also its availability.\nI hope this text has helped you understand what a load balancer is and some of the problems this solution aims to solve. Finally, I leave some links that were useful to me during the preparation of this text.\nReferences What Is Layer 4 Load Balancing? What is a Load Balancer? Load Balancing Definition Load balancing (computing) ","ref":"/posts/2022-02/load-balancing-what-is-a-load-balancer/"},{"title":"Build tests using Kotlin, JUnit and MockK","date":"","description":"","body":"The main objective of building code tests in an application is to certify what was coded, that is, to guarantee that a given piece of code does what it should do.\nJUnit is one of the most used frameworks for building tests in Kotlin and MockK to build mocks, which would be like doubles of an object and have the function of simulating the behavior of a component.\n💡 The codes used in this text are available on GitHub: https://github.com/rapatao/blog-koltin-junit-mockk Dependências utilizadas It is important to say that there are several ways to add support to the language and frameworks that we will use in this text. In the example below, only one of them will be presented, being basically how IntelliJ IDEA initializes projects in Kotlin + Gradle.\nplugins { id \u0026#34;org.jetbrains.kotlin.jvm\u0026#34; version \u0026#34;1.6.10\u0026#34; } ... dependencies { implementation(\u0026#34;org.jetbrains.kotlin:kotlin-stdlib\u0026#34;) testImplementation(\u0026#34;org.junit.jupiter:junit-jupiter:5.8.2\u0026#34;) testImplementation(\u0026#34;io.mockk:mockk:1.12.2\u0026#34;) } ... test { useJUnitPlatform() } Before adding these settings to your project, it is always important to verify that they are not already present in your project. This can be done through the task :dependencies, either through an IDE or through a terminal, with the following command:\n$ gradle dependencies With the output of the command, just look for the dependencies, if you find them, your configuration is correct.\nThe code to be tested As the main idea is to demonstrate the construction of tests using Kotlin, JUnit and MockK, the code used is extremely simple, but through it will be possible to demonstrate not only the creation of tests using JUnit, but also the construction mocks through MockK, among other details that we will detail throughout the text.\nCalculatorService.kt\nclass CalculatorService { fun sum(a: Int, b: Int) = a + b fun multi(a: Int, b: Int) = a * b } OpType.kt\nenum class OpType { SUM, MULTI } MainService.kt\nclass MainService( private val calculatorService: CalculatorService ) { fun execute(a: Int, b: Int, op: OpType) = when (op) { OpType.SUM -\u0026gt; calculatorService.sum(a, b) OpType.MULTI -\u0026gt; calculatorService.multi(a, b) } } The first test The declaration of tests with JUnit is done through the annotation org.junit.jupiter.api.Test added to a function that describes the scenario to be executed, as in the example:\nimport org.junit.jupiter.api.Test internal class ClassTest { @Test fun test() { // test block } } In general, every test verifies that something went as expected. There are several ways to do this, but they are usually done through the existing methods in the org.junit.jupiter.api.Assertions class, such as assertEquals. Other methods exist, and can be checked here.\nBased on the code presented above, we can create some test scenarios, but I will describe only two, which will basically perform simple tests with the SUM and MULTI operations.\nimport org.junit.jupiter.api.Assertions.assertEquals import org.junit.jupiter.api.Test internal class SimpleTest { @Test fun `o resultado de 2+3 deve ser 5`() { val calculatorService = CalculatorService() val mainService = MainService(calculatorService) val result = mainService.execute(2, 3, OpType.SUM) assertEquals(5, result) } @Test fun `o resultado de 2*3 deve ser 6`() { val calculatorService = CalculatorService() val mainService = MainService(calculatorService) val result = mainService.execute(2, 3, OpType.MULTI) assertEquals(6, result) } } Despite being simple, these tests demonstrate how the construction of tests is carried out, which basically consists of creating the necessary instances, invoking the function to be tested and comparing its result.\nReducing duplicate code in tests As can be seen, both scenarios perform the construction of an instance of the class to be tested and, consequently, of its dependencies. With JUnit, such cases could be constructed by declaring a specific annotation function, which is executed before or after one or all of the declared test scenarios.\nThese annotations are used when we need to prepare or remove data before or after the execution of test scenarios, such as, for example, inserting data into a database, or deleting information inserted in this same database.\nExisting annotations and their behavior are described below:\norg.junit.jupiter.api.BeforeAll: Runs before all test scenarios org.junit.jupiter.api.AfterEach: Runs before each test scenario org.junit.jupiter.api.AfterEach: Run after each test scenario org.junit.jupiter.api.AfterAll: Run after all test scenarios Knowing these annotations, we can rewrite the previous scenarios as follows:\nimport org.junit.jupiter.api.Assertions import org.junit.jupiter.api.BeforeEach import org.junit.jupiter.api.Test internal class WithBeforeTest { private lateinit var calculatorService: CalculatorService private lateinit var mainService: MainService @BeforeEach fun setup() { calculatorService = CalculatorService() mainService = MainService(calculatorService) } ... } Simulating calls on other classes Many tests can be built using \u0026ldquo;real\u0026rdquo; dependencies, that is, with their instances, however, in some cases this may not be possible, since these instances may need or access resources that are not available during the execution of the tests.\nIn these cases, we use tools that create mocks, which can be understood as instances that simulate the behavior of a real instance. This simulation is normally declared explicitly and would be something like: \u0026ldquo;when method A is invoked with certain parameters, B should be returned\u0026rdquo;.\nUsing our example classes, we can write the tests as follows:\nimport io.mockk.every import io.mockk.mockk import org.junit.jupiter.api.Assertions import org.junit.jupiter.api.BeforeEach import org.junit.jupiter.api.Test internal class WithBeforeTest { private lateinit var calculatorService: CalculatorService private lateinit var mainService: MainService @BeforeEach fun setup() { calculatorService = mockk() mainService = MainService(calculatorService) } @Test fun `o resultado de 2+3 deve ser 5`() { every { calculatorService.sum(any(), any()) } returns 5 val result = mainService.execute(2, 3, OpType.SUM) Assertions.assertEquals(5, result) } @Test fun `o resultado de 2*3 deve ser 6`() { every { calculatorService.multi(any(), any()) } returns 6 val result = mainService.execute(2, 3, OpType.MULTI) Assertions.assertEquals(6, result) } } Note that now, before invoking the execute method, we say how mock should behave when consumed. Although it doesn\u0026rsquo;t make much sense in our example, considering the simplicity of our code, this can be extremely useful when we need to simulate the use of a third party SDK that doesn\u0026rsquo;t provide the means to create tests, which could make it impossible to create tests , if not used mocks.\nCreating mocks with annotations The MockK framework provides a set of annotations that can be used to create mocks and inject them into the class to be tested, without this process being explicitly performed. This feature is useful when we need to create several mocks to build the tests and its construction is done by adding io.mockk.impl.annotations.MockK and io.mockk.impl.annotations.InjectMockKs annotations to the variables. declared in the test class.\nAfter that, we must change the setup method to initialize these variables, as we can see below:\nimport io.mockk.MockKAnnotations import io.mockk.every import io.mockk.impl.annotations.InjectMockKs import io.mockk.impl.annotations.MockK import org.junit.jupiter.api.Assertions import org.junit.jupiter.api.BeforeEach import org.junit.jupiter.api.Test internal class WithMockKAnnotationTest { @MockK private lateinit var calculatorService: CalculatorService @InjectMockKs private lateinit var mainService: MainService @BeforeEach fun setup() { MockKAnnotations.init(this) } ... } Reducing code when creating mocks JUnit offers an interesting feature called Extensions. With this feature, we can extend the behavior of the test framework, delegating various behaviors that may be necessary for the execution of test scenarios.\nPopular frameworks like Spring, via @SpringBootTest and Micronaut, with @MicronautTest make use of this feature to initialize the context before running scenarios.\nThe MockK framework also supports this feature, however, not using a specific annotation, but by an explicit declaration of the JUnit feature, which consists of adding the following annotation to the tests class:\n@org.junit.jupiter.api.extension.ExtendWith(MockKExtension::class) With its use, we no longer need, in our test class, we no longer need to declare the setup method, leaving our test class as follows:\nimport io.mockk.every import io.mockk.impl.annotations.InjectMockKs import io.mockk.impl.annotations.MockK import io.mockk.junit5.MockKExtension import org.junit.jupiter.api.Assertions import org.junit.jupiter.api.Test import org.junit.jupiter.api.extension.ExtendWith @ExtendWith(MockKExtension::class) internal class WithMockKExtensionTest { @MockK private lateinit var calculatorService: CalculatorService @InjectMockKs private lateinit var mainService: MainService ... } Conclusion In this text we discuss the dependencies necessary for building tests with Kotlin, using JUnit and MockK tools. We also describe how to build methods, executed before and after test cases, as well as creating mocks for cases where we cannot use a real implementation.\nI hope it helped you understand how to build tests using these tools, as well as how to optimize their construction, demonstrating how to reduce the amount of code needed to build test cases.\nThanks for reading and feel free to ask questions about it.\n","ref":"/posts/2022-01/build-tests-using-kotlin-junit-mockk/"},{"title":"Optimizing tests in Spring Boot applications","date":"","description":"","body":"It is extremely common for applications built with Spring Boot to have all their tests annotated with @SpringBootTest, but it is rare to find people who know the functionality of this annotation, when it is necessary to use it and its impact on the execution of an application\u0026rsquo;s tests.\nYou probably thought that this annotation is for building tests, but do you know what it represents, what happens when it is used?\nSimply put, this annotation is a simplified way of adding an extension to JUnit that aims to initialize the Spring context before the scenarios run of declared tests. With its use, before a test class is executed, the application is searched for the class annotated with @SpringBootApplication to identify possible customizations, loaded the settings for the test profile (when not overwritten) and only after the context be initialized, we have the method with the scenario executed.\nWith this in mind, it is interesting to know how to identify the possible impacts that its use can bring to projects.\nThe impact of the @SpringBootTest annotation We can consider this annotation a facilitator for the creation of tests, however, its indiscriminate use can add considerable execution time, since, as described above, several operations are performed before the tests are executed.\nWhen an application has few test scenarios, normally this time will not be noticed, but as applications and application complexities grow, several new test scenarios tend to be included, making the execution time increasingly impactful.\nFor reference, an application was created with only one component called **that will return a fixed text when the method is consumed. This application can be summarized in the following classes:\n@SpringBootApplication public class TempApplication { public static void main(String[] args) { SpringApplication.run(TempApplication.class, args); } } @Service public class TempService { public String get() { return \u0026#34;Nothing\u0026#34;; } } Normally, the tests for the TempService class would be implemented as follows:\n@SpringBootTest class TempApplicationTests { @Autowired private TempService service; @Test void test() { assertEquals(\u0026#34;Nothing\u0026#34;, service.get()); } } Like using the @SpringBootTest annotation, we have the Spring context initialized on each test and this simple scenario runs at 233ms. This number may seem small, but we are considering only a test class with only one scenario built and we must always remember that real applications tend to have dozens of classes with hundreds of test scenarios, which, theoretically, would make the total time of execution is the multiplication of this quantity.\nHow can we optimize tests? As you can see, the previous test doesn\u0026rsquo;t use any Spring resources, we were just using it to provide us with an instance of the TempService class, something that we could easily solve. For this specific case, we can simply remove all Spring annotations and instantiate our class manually, where we would have the following test class:\nclass TempApplicationTests { private TempService service = new TempService(); @Test void test() { assertEquals(\u0026#34;Nothing\u0026#34;, service.get()); } } With this change, we were able to get the same test coverage, but our test finishes with just 15ms, representing a runtime 15 times less than when using annotations.\nConclusion As we can see, simple scenarios, where we just use Spring to inject our components, we can easily replace the use of test annotations with native Java resources. With this we can drastically reduce the execution time of our tests and when we scale this to real applications, with different scenarios, we can notice a significant drop in the total execution time of the tests.\nHowever, it is important to clarify that this is not a solution that should be applied to all cases, as your application can and probably should have scenarios in which it would be necessary to use the context for the construction and execution of the tests. As an example for these cases, we can mention tests that aim to identify if all dependencies for our classes have been provided, tests of mappings and HTTP requests or even connections to databases.\nAnyway, this approach should not be followed for all cases, but it can certainly be applied in several of them and consequently would bring a reduction in the total time of execution of the tests.\n","ref":"/posts/2021-06/optimizing-tests-in-spring-boot-applications/"},{"title":"Thinking in GraalVM? Should I use Quarkus or Micronaut?","date":"","description":"","body":"Recently at work, we intended to use GraalVM in a new application. Looking for compatible solutions, we saw that Quarkus and Micronaut looked promising and decided to analyze them.\nNote: When this analysis was performed, Spring had very limited support for GraalVM and was disregarded.\nAmong the options mentioned, several benchmarks available on the Internet were consulted, in addition to verifying the compatibility with tools and libraries normally used in the company\u0026rsquo;s projects. Other factors were analyzed, such as ease and quality of documentation and learning curve in these technologies.\nLearning curve Despite being often disregarded in analyzes by technology people, the complexity of learning and, in fact, understanding how a technology works greatly affects the quality and productivity of the team on a project. Considering also that we intended to use GraalVM, it was extremely prudent to take this topic into consideration, as it would not just be a \u0026ldquo;new\u0026rdquo; technology to be implemented in the same project.\nThe team members also had a good knowledge of Spring, which is what is normally used in the developed projects, whether using the traditional MVC or the reactive WebFlux.\nAmong the projects that we normally develop, we rarely have a high complexity, in the sense of integration to the framework, and we usually only provide a API Rest that can consume other APIs and one or more databases, that is , our complexity tends to be in the business rules.\nBearing in mind the aforementioned points, both Quarkus and Micronaut have several similarities with Spring, and would facilitate many of our use cases, but as everything is declared in Micronaut (beans, * configurations*, etc) seemed to be much more similar to Spring, which could help in the tool assimilation process.\nAnother point that also brought some ease with Micronaut was the support that it has the declaration of queries in a declarative way, similar to the way available in Spring Data or GORM from Grails.\nDuring this review, we confirmed that Quarkus tends to follow or prioritize the use of specs, such as JAX-RS, for example, tending to not be the norm for those in the Spring universe.\nCompatibility As mentioned before, our use cases are rarely very complex, being limited to analysis in the most common scenarios, that is, availability and consumption of a REST APIs, as well as access to databases, usually MySQL.\nThe availability of endpoints, both tools have similarities, focusing on the fact that Quarkus uses by default JAX-RS annotations, while Micronaut, despite having support for them, uses its own annotations, but these are very intuitive. An important point, and much neglected when we talk about APIs, would be their documentation and in this matter, both have native support for OpenAPI, allowing a documentation to be generated based on the declaration of endpoints existing in the code (code first).\nJust as it is simple to provide an endpoint, creating an HTTP client is also similar in both frameworks, in which the client is built only with the creation of an annotated interface, making explicit in its methods, the verb, the path, its parameters, its headers and the request body, very similar to the already known Feign and Retrofit.\nA point that can be considered interesting in Micronaut is in relation to the (de)serialization settings, where they can be defined in the configuration file, without the need for coding. Through this file it is possible to define from which naming strategy to use (camel case, snake case, etc), to how the data should be converted while in Quarkus, this can only be done by encoding, increasing the amount of code produced/contained in the project.\nDatabase access is similar, with both having Hibernate support and native support for multiple databases. The issue is how the queries that will be performed will be declared, with Micronaut supporting declarations through interfaces and descriptive methods, while Quarkus, as far as I could identify, performs in a more declarative and, consequently, verbose way.\nIn this project in question, we needed to perform a kind of versioning of all entities that were changed in the database, however, the history is stored in a service external to the application. The simplest way to perform this integration would be through an interceptor in Hibernate. With Micronaut, this integration is extremely simple, just having a bean of this type available in the context, which is automatically used. Regarding Quarkus, I couldn\u0026rsquo;t identify how this integration would be carried out, nor could I find it, either on the internet or in its official documentation.\nDocumentation I consider this one of the most important topics to be analyzed, because the documentation is and should always be used as a reference to analyze or build possible use cases, or just to understand the features provided by a tool.\nAt this point, I believe that Quarkus leaves a lot to be desired, as its documentation is based on practical guides and, particularly, I consider this very bad for a long-term project, but they are excellent for those who would like to test some integration or functionality. In other words, during the development of a project, the tendency is for research related to technology to change from \u0026ldquo;how to do something\u0026rdquo; to \u0026ldquo;why something happens\u0026rdquo;. With the separation by tabs, this search becomes difficult and tiring, which would make people look for other means, like Stack Overflow, for example. As an example, I can mention the need mentioned above, to create an Interceptor, and I couldn\u0026rsquo;t find examples of usage in the guides and even in search engines, I couldn\u0026rsquo;t find anything related to it.\nOn the other hand, with Micronaut, its documentation is much more detailed and complete, I believe it lacks a lot to reach the level of content that the Spring documentation has, but it is possible to find practically everything related to a topic of centrally or in specific documentation for an extension, greatly reducing the need to open and read several guides on the same subject to find something specific.\nConclusion Both frameworks have support and compatibility with the scenarios we analyzed and could be easily adopted in any of our projects. However, Micronaut\u0026rsquo;s learning curve proved to be much smoother, with many similarities with the already used Spring, such as support for accessing databases through the declaration of interfaces and descriptive methods.\nHowever, the factor that I believe had a greater weight in the decision was the documentation, which despite believing that it can still be evolved, in Micronaut it is much more complete and easier to find than the one found with the guides provided by Quarkus .\nWith all that said, if I were to leave the Spring universe, whether to adopt GraalVM, today I would choose Micronaut, without any great fears in this decision.\nThanks for reading and feel free to ask any questions you may have.\n","ref":"/posts/2021-03/thinking-in-graalvm-should-i-use-quarkus-or-micronaut/"},{"title":"Build virtual machines (VM's) using QEMU","date":"","description":"","body":"Recently I needed to perform some simple tests of an application that only worked on Windows and as I don\u0026rsquo;t have this environment, I decided to create a VM with Windows 10. My first idea was to use VirtualBox , however, for some reason, I couldn\u0026rsquo;t start the installer at all, always generating a random error or else the screen was like the hiss of a TV, but colorful and without sound.\nThis problem prevented me from proceeding with the installation and almost made me give up on the test I was going to perform, however, I decided to test create a VM using QEMU.\nAlthough there are interfaces (GUI) to configure VM\u0026rsquo;s for QEMU, I didn\u0026rsquo;t find any of the alternatives I know in Homebrew (aqemu, virt-manager) and to carry out my test, I decided to do the procedure manually.\nWhat is QEMU It can be said that QEMU is an open source, cross-platform application that serves to emulate machines (VM) and virtualize instructions from other platforms, such as, for example, emulating ARM and PowerPC instructions in x86 environments.\nInstalling QEMU Installation can be done in several ways, all of which can be found here. In this text, I will describe how to install on macOS using Homebrew and Linux systems, which are based on Debian/Ubuntu.\n# macOS brew install qemu # Debian/Ubuntu apt-get install qemu Creating an operating system installation disk We use the qemu-img command for this, for example:\nqemu-img create -f qcow2 windows10.qcow2 30G In the above command, we are creating a file, called windows10.qcow2 with 30G. This file will be used to install Windows 10.\nInstalling the operating system on the created disk At this point, we must pay attention to the system that will be virtualized, being necessary to execute the command accordingly. For example, if the installation is Windows 10 64-bit, the command should be qemu-system-x86_64, in case the environment is ARM-based, the command would be: qemu-system-arm.\nFor my scenario, I will install Windows 10 64-bit, so the command I will run will be the following:\nqemu-system-x86_64 -hda windows10.qcow2 -m 4G -cdrom ~/Downloads/tibia/Win10_2004_EnglishInternational_x64.iso -boot c In the above command, QEMU will simulate a 64-bit environment, with 4G of RAM, using the file created previously as a disk and the ISO file, downloaded from Microsoft\u0026rsquo;s website, which contains the installation image, will be presented to the system as if it were a CD.\nBooting the installed system After the whole procedure, if you do not close QEMU, you will normally access your virtual machine, however, after closed, if you want to access again, just run the following command:\nqemu-system-x86_64 -m 4G -hda windows10.qcow2 Conclusion As seen, related to virtualization, there is a free alternative to VirtualBox that can often meet our needs for creating and running virtual machines. Despite its greater complexity, knowing QEMU can be useful for situations where we don\u0026rsquo;t have a friendlier solution to create our VMs.\nTo learn more about QEMU, I recommend looking at the official Wiki: https://wiki.qemu.org/Main_Page\n","ref":"/posts/2020-09/build-virtual-machines-using-qemu/"},{"title":"A list of Brazilian projects","date":"","description":"","body":"Every developer, at least once, must have come across awesome lists calls. These lists try to bring together in one place projects related to a theme or libraries for which they were developed.\nSome examples of these lists are:\nawesome-java awesome-python vertx-awesome awesome-javascript awesome-go There are several other lists of this type, just search on GitHub and you will probably find one related to the topic you want. See several lists here.\nThese lists are very useful and serve as a basis for us to find solutions and tools to solve possible problems that we are facing or simply to facilitate some of our daily activities.\nClaudison, o Filho da Nuvem introduced me to the repository of Felipe Fialho with the purpose of grouping Brazilian projects.\nSee the list: awesome-made-by-brazilians Here is my invitation, if you know Brazilian projects and believe that they deserve to be highlighted, feel free to open your PR in Felipe\u0026rsquo;s repository, I believe he won\u0026rsquo;t mind it!\nEdit: I had created a repository for this purpose, but there is no reason to have two lists with the same purpose! Let\u0026rsquo;s collaborate with what already exists!\n","ref":"/posts/2020-08/a-list-of-brazilian-projects/"},{"title":"ACID Transactions","date":"","description":"","body":"ACID transactions Acronym for Atomicity, Cconsistency, Iinsulation and Durability is a set of properties of a database transaction that aim to ensure data integrity and validity, even after systemic failures or electrical power failures.\nAtomicity It treats as a unit, all operations performed for a task, that is, all instructions will be successfully carried out or, none of them should be carried out. For example, in a bank transfer, in case of failure in any of the steps, the entire process must be undone and the values returned to the original.\nConsistency Data is considered to be consistent and should be kept that way even after other transactions. That is, in a bank transfer process, if the deposit step fails, the data must return to its initial state, with the amount returned to the source account. The base state should only be changed if all operations complete successfully.\nIsolation Determines when and how values changed by one transaction will be available for other transactions. For example, in a case where a value is being changed and queried simultaneously, the system must guarantee that the value returned is exactly the result of the change transaction, not returning data different from the actual one. This ensures that, in case the write operation fails, the value returned to the query is the original value and not what it was supposed to be after the write operation was completed.\nDurability It guarantees that a data, when persisted, will be available even after a failure, such as, for example, a fatal error in the application or even a power outage. That is, the storage of these values is done effectively, guaranteeing their availability after reestablishment of services.\nConclusion Basically, ACID aims to guarantee that transactions carried out in a system are always carried out in full, guaranteeing data availability even after critical failures, such as power outages. It also aims to ensure that in a high concurrency system, the values consulted are always consistent with the values persisted in the base, avoiding the so-called dirty reads, where a data is returned with a pre-effective value of a write, which may or may not occur. successfully.\n","ref":"/posts/2020-08/acid-transactions/"},{"title":"Your dotfiles under control","date":"","description":"","body":"Many developers throughout their career end up creating many scripts, aliases and customizations in their development environments. These settings are normally kept in files, and when this user changes their environment, they sometimes make a copy of these files to the new environment. This process tends to be done by copying your files from one environment to another.\nAlthough it works, there is a more practical way to manage these files, making the proper associations as needed, including supporting different profiles. That is, it is possible to create a dotfile with different profiles, defining different settings according to your needs. An example of this use would be to build a profile for your personal environment, another for your professional environment, changing server access keys and commits signature settings in git.\nWhat are dotfiles On Unix-based systems, it is common for settings to be kept in files starting with a period (.). These files or folders are considered to be “hidden” and are not normally listed using commands such as ls, unless specified to be displayed.\nExamples of files and folders starting with a dot (.):\n.gitignore .m2 .bashrc .zshrc This doesn\u0026rsquo;t mean that your files need to start with a period (.) and in fact there are many people who don\u0026rsquo;t define it that way, separating them into folders with matching names, like, for example, “$HOME/etc”. Particularly, I prefer to keep such files inside a folder starting with a period (.), specifically “$HOME/.dotifiles.d”, as this folder is not listed by default, so it will not be noticed most of the time, generating in me a sense of organization.\nA dotfiles manager As said before, there are several ways to manage your dotfiles, either manually or automatically with the help of applications. Using a version controller, such as git is an interesting way to do it, however, we will still need to perform the links manually in each installed environment.\nAlthough it is possible to version your $HOME, by doing this, by default, your list of ignored files would be extremely long, in addition to the risk of adding inappropriate files to our repository. Using the versatility that git offers us, but without the difficulty of having to add several files and directories to our gitignore, yadm (Yet Another Dotfiles Manager) helps us in the task of managing our settings, with a number of interesting additional features.\nAs a kind of \u0026ldquo;shell\u0026rdquo; of Git, with yadm you will be able to version your $HOME in a simplified way, with the ability to add encryption to sensitive files, such as access key, use alternative files according to the environment which your dotfiles are being used, in addition to knowing exactly what has changed before updating your settings repository.\nInstalling yadm There are several ways to install yadm, from using package managers like Homebrew and apt-get to downloading the file to a folder on your device. All these ways can be found in the official documentation here.\nEach operating system offers a way to install, which ends up facilitating the yadm update process and below is the installation method with greater compatibility, that is, regardless of whether your operating system is macOS, Ubuntu, Arch Linux or FreeBSD.\n$ curl -fLo /usr/local/bin/yadm https://github.com/TheLocehiliosan/yadm/raw/master/yadm \u0026amp;\u0026amp; chmod a+x /usr/local/bin/yadm yadm requires that Git be installed on your system and when trying to run without this dependency, the following error message will be displayed:\n$ yadm version /usr/local/bin/yadm: line 909: git: command not found ERROR: This functionality requires Git to be installed, but the command \u0026#39;git\u0026#39; cannot be located With Git installed the message should be as follows:\n$ yadm version yadm 2.5.0 Adding your first files The commands used to start a repository, add files, perform commit as well as send and download updates from a repository are the same commands available in Git, only started with yadm.\nTo start your first repository, the following command must be run:\n$ yadm init Initialized empty shared Git repository in /home/rapatao/.config/yadm/repo.git/ Once initialized, you can add all the files that you consider important and should be versioned, this usually includes files like, for example: .bashrch and .zshrc\nTo add files, just run the command below, where should be replaced by the file you want to version.\n$ yadm add \u0026lt;file\u0026gt; After adding the files, it\u0026rsquo;s time to perform your first commit and for that, just run the following command:\n$ yadm commit -m \u0026#34;primeiro commit\u0026#34; [master (root-commit) 506b780] primeiro commit 1 file changed, 117 insertions(+) create mode 100644 .bashrc If your Git is not correctly configured, the message below will be displayed, informing you that your email and name were not configured, preventing your commit from being performed. To solve it, just run the command as instructed in the message and repeat the procedure, thus achieving your commit.\n$ yadm commit -m \u0026#34;primeiro commit\u0026#34; *** Please tell me who you are. Run git config --global user.email \u0026#34;you@example.com\u0026#34; git config --global user.name \u0026#34;Your Name\u0026#34; to set your account\u0026#39;s default identity. Omit --global to set the identity only in this repository. fatal: unable to auto-detect email address (got \u0026#39;rapatao@89122a756847.(none)\u0026#39;) Your settings in a remote repository One of the advantages of Git is that you can have a local repository and a copy of this repository remotely. This is extremely helpful in ensuring that your files can be accessed from virtually anywhere.\nTo upload your files to a remote repository, you can use services available on the Internet, such as GitHub, BitBucket or GitLab. The procedure is similar to the one performed with the Git command, only a remote repository needs to exist. Example:\n$ yadm remote add origin \u0026lt;url\u0026gt; $ yadm push -u origin master Your settings in a new environment With the advent of the decentralized repository, to install your files in a new environment, just run the following command:\n$ yadm clone \u0026lt;url\u0026gt; The command will clone the repository locally and the versioned files will be copied to the appropriate locations, that is, everything you configured will be automatically replicated to this new environment and all you had to do was install yadm.\nIt is important to note that when performing the clone, yadm will try to perform a merge with the pre-existing files in the new environment, which may cause conflicts or even fail in the process. Normally these files will, in fact, be replaced by the content you previously versioned, so the simplest way to solve such a problem is to delete such existing files locally before performing the clone.\nDifferent systems with different files One of the great advantages of yadm is the possibility of having files with different configurations and these being used according to a pre-defined rule. For example, it is possible to create Git configuration files and, depending on the environment, one of these files will be used.\nLet\u0026rsquo;s assume that you use macOS at home and Linux at your company. When you are at home, you would like to have your commits signed with the email “luiz@example.com” and when you are at the company with “luiz.empresa@example.com”.\nTo do this, just create the files with the appropriate differences and save them using the following pattern:\n\u0026lt;file\u0026gt;##\u0026lt;condition\u0026gt;[,\u0026lt;condition\u0026gt;,...] The full list of conditions can be found here.\nIn the scenario reported above, we will have the following files:\n$ ls -a .gitconfig* .gitconfig##os.Darwin .gitconfig##os.Linux Now we just manage these files with yadm and that\u0026rsquo;s it, a symbolic link will be created pointing to the file for which the condition is true.\n$ yadm add .gitconfig* $ ls -la .gitconfig* lrwxrwxrwx 1 rapatao rapatao 20 Aug 15 17:25 .gitconfig -\u0026gt; .gitconfig##os.Linux -rw-rw-r-- 1 rapatao rapatao 57 Aug 15 17:18 .gitconfig##os.Darwin -rw-rw-r-- 1 rapatao rapatao 57 Aug 15 17:18 .gitconfig##os.Linux As we can see, when we added the file, yadm automatically created a symbolic link pointing from \u0026ldquo;.gitconfig\u0026rdquo; to \u0026ldquo;.gitconfig##os.Linux\u0026rdquo;.\nConclusion As we can see so far, using yadm can help control our settings, bringing together all the potential that Git offers us, such as change control and content distribution, as well as several other features, such as the example above , where we “customize” our settings based on the operating system we are running.\nIn addition to the resources mentioned above, there are others and although I do not describe them in this text, I believe they can be interesting for almost all users:\nEncryption: Encrypts sensitive content. Useful for versioning SSH keys, since to decode, you will need to supply a user-defined password. Bootstrap: A script that can be run when cloning a repository. Useful to install applications normally used and/or necessary for the correct functioning of your environment. I believe I have talked about the main topics about dotfiles and how to manage them using yadm. For more details, I recommend checking the official website yadm.io, but feel free to ask me something directly about it.\n","ref":"/posts/2020-08/your-dotfiles-under-control/"},{"title":"Reactive programming with Spring WebFlux, should I use it?","date":"","description":"","body":"Should I use it?\nNo, but maybe so! I could stop the article here, but that wouldn\u0026rsquo;t add anything and you would never understand why I start the text with a no, but it may be yes. The truth is, if you hope to use WebFlux and solve all your problems, I\u0026rsquo;m sorry to tell you that your problems may be just beginning.\nRecently the subject Advantages and Disadvantages about WebFlux came up in my work and because I have little experience using WebFlux, I decided to seek more information on the subject and I intend to describe here my opinion about the use of WebFlux and, consequently, a little about reactive programming.\nReactive programming has been on the rise for some years and there is a lot of talk about performance gains, resource optimization, but the discussion on the subject is still very shallow and sometimes with an air of pure fad.\nWhen researching the performance of reactive applications, we end up seeing wonderful graphics, as if all problems were solved with a simple paradigm shift, but often we don\u0026rsquo;t notice that the comparisons turn out to be a little strange when compared to \u0026ldquo;real\u0026rdquo; applications. .\nI am referring to the fact that these tests consider a complete migration of the application architecture, that is, from the code to the database, and this is usually not an option, as we cannot always replace our database, for example, from MySQL to MongoDB or Cassandra. This, for me, makes the results of these comparisons to be seen with a certain weight.\nHowever, before making any changes to the code, we must always answer some questions that I consider fundamental for the success of tasks at this level.\nIs the team aware of the technology that will be used? If not, will the team have time to learn the new technology? Does the team understand the advantages, disadvantages and risks of this technology? Is the new technology compatible with the other tools used? Eg: Database. It is always important to answer these questions before starting a project with the adoption of a different technology than usual. This helps to prevent unexpected risks and possible delays in deliveries.\nWe developers often have the desire to use something new/different in our day-to-day tasks, but it is always important to consider what this will add, with difficulty, learning curve and inherent risks to this adhesion.\nBack to the main subject, Spring WebFlux\u0026hellip;*\nAmong everything I read and had the opportunity to see in practice, I found the efficiency of applications built with WebFlux very interesting. It was the famous “a lot with a little”, but what caught my attention was the complexity of building unit/integrated tests as well as the difficulty in performing debug, since the call flow is no longer, so to speak, sequential.\nAmong the various articles I read, there were several reports of people who categorically stated that WebFlux was better, as it was possible to process requests in parallel, while in the model used by Spring MVC, only one request was performed at a time , which is false, as parallelism is created with threads and controlled by application servers, such as Tomcat and Jetty.\nAnyway, should I use WebFlux in my project?\nWell, that depends\u0026hellip;\nThe fact that the application is built using a reactive paradigm (WebFlux) does not make it perform better than the imperative model (Spring MVC). Often the performance problem can be simply in the way things were implemented or even by consuming services that block a transaction for too long. There are other scenarios that could improve the performance of an application without first considering a technology migration, such as, for example, parameterization of the JVM.\nIn short, if today an application works well using Spring MVC, there \u0026ldquo;are\u0026rdquo; no real reasons to migrate to WebFlux, besides the fact that, depending on the problem, there may be another way to solve it, without first consider a technology migration.\nHowever, if the application is having problems related to the amount of threads, or uses streaming of data, or even prioritizes an efficient use of hardware resources, perhaps the use of WebFlux is a way out.\nAnyway, there is no magic formula that will tell whether or not we should use WebFlux, or MVC in our projects. It will always be necessary to analyze the scenario we have and compare it with the options we have available, but it is always important to understand the advantages and disadvantages of these and other technologies.\nA migration may not be necessary now, but in the future it may be and it\u0026rsquo;s always good to be prepared.\n","ref":"/posts/2020-07/should-i-use-spring-webflux/"},{"title":"Verify vararg methods using Mockito","date":"","description":"","body":"Building tests is one of the most routine tasks for a developer and normally, with frameworks like jUnit and Mockito, this task tends to be carried out without great difficulties. Despite this, there are certain validations, which tend to be more complex, such as checking calls to methods with parameters of type varargs.\nIf you don\u0026rsquo;t know what varargs is, you can roughly say that it is, the \u0026ldquo;\u0026hellip;\u0026rdquo; used in parameter declarations in methods, however, I recommend reading this article for better understanding and viewing examples.\nDespite not being highly complex, we do not always have control over how implementations will be invoked, especially when we use frameworks that make use of concepts such as callbacks or even reflection, widely used in the Spring framework.\nConsider the following class declaration:\nclass Varargs { public void consume(String... values) { System.out.println(String.join(\u0026#34;, \u0026#34;, values)); } } We must consider that the Varargs class will be invoked by some framework over which we have no control, however, we need to ensure that, knowing the input value, the values ​​to be received in the implementation will also be known.\nTo try to exemplify a scenario, we can consider that the framework will convert values ​​from the consumption response of a REST API, perform calculations and consume the implementation with the result of this operation.\nAlthough we cannot control the operations performed by this framework, we can manipulate the execution, sending known values ​​and validating if the implementation\u0026rsquo;s consumption will be done as expected.\nThis type of test is very useful to guarantee that updates of this framework, do not compromise the behavior of our application, allowing constant and safe updates.\nImagining that the framework in use has a class that we cannot control, exemplified below by the Invoker class with final modifier, we must ensure that after the operation, when the method of the Varargs class is invoked, the same arguments are always received, based on the input parameters.\nfinal class Invoker { private final Varargs varargs; public Invoker(Varargs varargs) { this.varargs = varargs; } public void invoke(String... values) { final var newValue = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(values)); newValue.add(0, String.valueOf(values.length)); varargs.consume(newValue.toArray(new String[0])); } } Considering everything that was said above, here are some examples of using our implementation by the Invoker class:\nfinal var varargs = new Varargs(); final var invoker = new Invoker(varargs); invoker.invoke(); // 0 invoker.invoke(\u0026#34;1\u0026#34;); // 1, 1 invoker.invoke(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;); // 2, 1, 2 Given the context, the following is a test implementation for scenario validation:\n@Test public void testNoArgument() { final var varargs = Mockito.spy(new Varargs()); new Invoker(varargs).invoke(); final var stringArgumentCaptor = ArgumentCaptor.forClass(String.class); verify(varargs).consume(stringArgumentCaptor.capture()); assertThat(stringArgumentCaptor.getAllValues(), hasSize(1)); assertThat(stringArgumentCaptor.getAllValues(), containsInRelativeOrder(\u0026#34;0\u0026#34;)); } @Test public void testSingleArgument() { final var varargs = Mockito.spy(new Varargs()); new Invoker(varargs).invoke(\u0026#34;1\u0026#34;); final var stringArgumentCaptor = ArgumentCaptor.forClass(String.class); verify(varargs).consume(stringArgumentCaptor.capture()); assertThat(stringArgumentCaptor.getAllValues(), hasSize(2)); assertThat(stringArgumentCaptor.getAllValues(), containsInRelativeOrder(\u0026#34;1\u0026#34;, \u0026#34;1\u0026#34;)); } @Test public void testTwoArguments() { final var varargs = Mockito.spy(new Varargs()); new Invoker(varargs).invoke(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;); final var stringArgumentCaptor = ArgumentCaptor.forClass(String.class); verify(varargs).consume(stringArgumentCaptor.capture()); assertThat(stringArgumentCaptor.getAllValues(), hasSize(3)); assertThat(stringArgumentCaptor.getAllValues(), containsInRelativeOrder(\u0026#34;2\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;)); } This way we can guarantee that when Invoker receives certain input parameters, Varargs will always be consumed with the same values.\nAlthough the example here is not from a real scenario, I believe that it can be easily adapted to various cases, thus serving as a reference.\nIf you have any questions or suggestions, don\u0026rsquo;t hesitate to get in touch!\n","ref":"/posts/2020-05/verify-varargs-using-mockito/"},{"title":"Experiences","date":"","description":"","body":" ","ref":"/experiences/"},{"title":"JSON Formatter & Minifier","date":"","description":"","body":"JSON Formatter \u0026amp; Minifier is a tool to validate, format or minify a string JSON.\nJSON: format minify clear IMPORTANT: This tool does not use any server-side data processing.\n","ref":"/json/"}]